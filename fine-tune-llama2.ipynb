{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Install All the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:36:23.015655Z",
     "iopub.status.busy": "2025-03-16T10:36:23.015379Z",
     "iopub.status.idle": "2025-03-16T10:36:42.088267Z",
     "shell.execute_reply": "2025-03-16T10:36:42.087365Z",
     "shell.execute_reply.started": "2025-03-16T10:36:23.015635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import All the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:37:53.807176Z",
     "iopub.status.busy": "2025-03-16T10:37:53.806863Z",
     "iopub.status.idle": "2025-03-16T10:38:27.032503Z",
     "shell.execute_reply": "2025-03-16T10:38:27.031614Z",
     "shell.execute_reply.started": "2025-03-16T10:37:53.807154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In case of Llama 2, the following prompt template is used for the chat models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Prompt (optional) to guide the model\n",
    "\n",
    "\n",
    "User prompt (required) to give the instruction\n",
    "\n",
    "\n",
    "Model Answer (required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will reformat our instruction dataset to follow Llama 2 template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Orignal Dataset: https://huggingface.co/datasets/timdettmers/openassistant-guanaco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reformat Dataset following the Llama 2 template with 1k sample: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete Reformat Dataset following the Llama 2 template: https://huggingface.co/datasets/mlabonne/guanaco-llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know how this dataset was created, you can check this notebook.  \n",
    "\n",
    "https://colab.research.google.com/drive/1Ad7a9zMmkxuXTOh1Z7-rNSICA4dybpM2?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How to fine tune Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Free Google Colab offers a 15GB Graphics Card (Limited Resources --> Barely enough to store Llama 2–7b’s weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We also need to consider the overhead due to optimizer states, gradients, and forward activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use LoRA here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load a llama-2-7b-chat-hf model (chat model)\n",
    "2. Train it on the mlabonne/guanaco-llama2-1k (1,000 samples), which will produce our fine-tuned model Llama-2-7b-chat-finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QLoRA will use a rank of 64 with a scaling parameter of 16. We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:47:03.685999Z",
     "iopub.status.busy": "2025-03-16T10:47:03.685693Z",
     "iopub.status.idle": "2025-03-16T10:47:03.692430Z",
     "shell.execute_reply": "2025-03-16T10:47:03.691430Z",
     "shell.execute_reply.started": "2025-03-16T10:47:03.685977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"/kaggle/working/results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:Load everything and start the fine-tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\n",
    "\n",
    "\n",
    "2. Then, we’re configuring bitsandbytes for 4-bit quantization.\n",
    "\n",
    "\n",
    "3. Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\n",
    "\n",
    "\n",
    "4. Finally, we're loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:48:37.103510Z",
     "iopub.status.busy": "2025-03-16T10:48:37.103177Z",
     "iopub.status.idle": "2025-03-16T11:18:38.069004Z",
     "shell.execute_reply": "2025-03-16T11:18:38.068281Z",
     "shell.execute_reply.started": "2025-03-16T10:48:37.103485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8580ec4b2ad14722bf398db96925154e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7444ddeb9942108e7e35746d1ce39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-9ad84bb9cf65a42f.parquet:   0%|          | 0.00/967k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a5c290f590416691a57e13142b334b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c59ad6f483a4c698d01e98b06b344f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99954c8035ce490c8bd74694a6182af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d89dded7f346ea99361766ee020537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cd6bd53e2343de8ddbff12baa905c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b36f036d6c46089bbdd732c56a5c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c4a661201a4b3dac40be97a1790fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6b30593a2f42b1a484e3bf7e9ce4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bc6d86d44948d792e58090729dad2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0658096839c24838995381fff4a6bb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b2173d7705439ead48abf204eac4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bd0a3ecb014a30ab5a99595210ff20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6206f0f2fe4e5895f4dce60bd89f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e224f5018b904a8bb56adbf1322b3f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 27:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.334200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.417600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=1.4165204162597655, metrics={'train_runtime': 1708.1518, 'train_samples_per_second': 0.585, 'train_steps_per_second': 0.073, 'total_flos': 8971066649149440.0, 'train_loss': 1.4165204162597655, 'epoch': 1.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:18:59.302795Z",
     "iopub.status.busy": "2025-03-16T11:18:59.302521Z",
     "iopub.status.idle": "2025-03-16T11:18:59.514173Z",
     "shell.execute_reply": "2025-03-16T11:18:59.513512Z",
     "shell.execute_reply.started": "2025-03-16T11:18:59.302776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Check the plots on tensorboard, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:19:02.959149Z",
     "iopub.status.busy": "2025-03-16T11:19:02.958760Z",
     "iopub.status.idle": "2025-03-16T11:19:12.097257Z",
     "shell.execute_reply": "2025-03-16T11:19:12.096545Z",
     "shell.execute_reply.started": "2025-03-16T11:19:02.959115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 6:Use the text generation pipeline to ask questions like “What is a large language model?” Note that I’m formatting the input to match Llama 2 prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:19:57.470799Z",
     "iopub.status.busy": "2025-03-16T11:19:57.470515Z",
     "iopub.status.idle": "2025-03-16T11:20:56.280555Z",
     "shell.execute_reply": "2025-03-16T11:20:56.279733Z",
     "shell.execute_reply.started": "2025-03-16T11:19:57.470779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is a large language model? [/INST] A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate human-like language.\n",
      "\n",
      "The model is trained on a large dataset of text, such as books, articles, or social media posts, and uses a combination of machine learning algorithms and statistical techniques to learn the patterns and structures of language. The model can then generate text that is similar in style and structure to the training data, and can be used for a variety of applications such as language translation, text summarization, and chatbots.\n",
      "\n",
      "Some of the most popular large language models include BERT, RoBERTa, and XLNet. These models have been trained on massive datasets of text and have achieved state-of-the-art results on a wide range of natural language processing (NLP) tasks.\n",
      "\n",
      "Large language models have many potential applications,\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:20:56.281751Z",
     "iopub.status.busy": "2025-03-16T11:20:56.281448Z",
     "iopub.status.idle": "2025-03-16T11:20:56.990948Z",
     "shell.execute_reply": "2025-03-16T11:20:56.990048Z",
     "shell.execute_reply.started": "2025-03-16T11:20:56.281727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train a Llama 2 model on the entire dataset using [mlabonne/guanaco-llama2](https://huggingface.co/datasets/mlabonne/guanaco-llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 7: Store New Llama2 Model (Llama-2-7b-chat-finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we store our new Llama-2-7b-chat-finetune model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:21:07.409660Z",
     "iopub.status.busy": "2025-03-16T11:21:07.409347Z",
     "iopub.status.idle": "2025-03-16T11:21:44.908582Z",
     "shell.execute_reply": "2025-03-16T11:21:44.907802Z",
     "shell.execute_reply.started": "2025-03-16T11:21:07.409637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad71deb5f6b48738c7ad23b23d1a89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 8: Push Model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:22:55.370016Z",
     "iopub.status.busy": "2025-03-16T11:22:55.369682Z",
     "iopub.status.idle": "2025-03-16T11:22:55.373872Z",
     "shell.execute_reply": "2025-03-16T11:22:55.373066Z",
     "shell.execute_reply.started": "2025-03-16T11:22:55.369987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:35:06.321396Z",
     "iopub.status.busy": "2025-03-16T11:35:06.321054Z",
     "iopub.status.idle": "2025-03-16T11:39:59.535120Z",
     "shell.execute_reply": "2025-03-16T11:39:59.534398Z",
     "shell.execute_reply.started": "2025-03-16T11:35:06.321365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8af65a5012040bea71357906f006841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae341e5edd3e40a086acb627d4a8356f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffd413d55104c549e76ae31697bce5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6a1051f9f3495a89c85eb181162098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ramdhan9604/Llama-2-7b-chat-finetune/commit/88a506fe6f14ce98577e05d14ae373478b76892b', commit_message='Upload tokenizer', commit_description='', oid='88a506fe6f14ce98577e05d14ae373478b76892b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ramdhan9604/Llama-2-7b-chat-finetune', endpoint='https://huggingface.co', repo_type='model', repo_id='ramdhan9604/Llama-2-7b-chat-finetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !huggingface-cli login\n",
    "\n",
    "# model.push_to_hub(\"ramdhan9604/Llama-2-7b-chat-finetune\", check_pr=True)\n",
    "\n",
    "# tokenizer.push_to_hub(\"ramdhan9604/Llama-2-7b-chat-finetune\",check_pr=True)\n",
    "\n",
    "model.push_to_hub(\"ramdhan9604/Llama-2-7b-chat-finetune\", use_auth_token=api_token, check_pr=True)\n",
    "tokenizer.push_to_hub(\"ramdhan9604/Llama-2-7b-chat-finetune\", use_auth_token=api_token, check_pr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
